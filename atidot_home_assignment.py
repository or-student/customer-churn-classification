# -*- coding: utf-8 -*-
"""Atidot_home_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngmCKHtScyja_1lb4jokh0U8chEGYjZC
"""

import matplotlib.pyplot as plt
import numpy as np
import datetime
from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV,KFold
from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder
from dateutil.relativedelta import relativedelta
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report,f1_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import statsmodels.api as sm
import shap
import pandas_datareader as pdr
import pickle

from google.colab import drive
import pandas as pd
drive.mount('/content/drive')
df_origianl = pd.read_csv("/content/drive/MyDrive/Atidot_home_assignment/churn_data.csv")

df = df_origianl.copy()
df_origianl.describe()

df_origianl.info()

grouping_by_customer = GroupShuffleSplit(n_splits=1,test_size=0.3,random_state=42)
train_gbc_index, test_gbc_index = next(grouping_by_customer.split(df,groups=df['customer_id']))
train_gbc = df.loc[train_gbc_index]
test_gbc = df.loc[test_gbc_index]

for df_gbc in [train_gbc,test_gbc]:
  df_gbc['date_ind'] = df_gbc['date'].copy()
  df_gbc['customer_lifetime'] = (pd.to_datetime(df_gbc['date']).dt.to_period('M') - pd.to_datetime(df_gbc['issuing_date']).dt.to_period('M')).apply(lambda x: x.n)
  df_gbc.set_index(['customer_id','date_ind'], inplace=True)

train_gbc_churn = train_gbc['churn']
train_gbc = train_gbc.drop('churn',axis = 1)
test_gbc_churn = test_gbc['churn']
test_gbc = test_gbc.drop('churn',axis = 1)

"""#separate date into year and month"""

def convert_date_2_year_and_month(df,col):
  df[col+'_year'] = pd.to_datetime(df[col]).dt.year
  df[col+'_month'] = pd.to_datetime(df[col]).dt.month
  return df

for date_col in ['date','issuing_date']:
  train_gbc = convert_date_2_year_and_month(train_gbc,date_col)
  test_gbc = convert_date_2_year_and_month(test_gbc,date_col)

# train_gbc.drop(['date','issuing_date'], axis=1, inplace=True)
# test_gbc.drop(['date','issuing_date'], axis=1, inplace=True)

def encode_months(df, month_col):
    df['month_rad'] = df[month_col].apply(lambda x: 2 * np.pi * x / 12)
    df[month_col+'_sin'] = np.sin(df['month_rad'])
    df[month_col+'_cos'] = np.cos(df['month_rad'])
    df.drop('month_rad', axis=1, inplace=True)
    return df

train_gbc

for m in ['date_month','issuing_date_month']:
  train_gbc = encode_months(train_gbc, m)
  test_gbc = encode_months(test_gbc, m)

# train_gbc.drop(['date_month','issuing_date_month'], axis=1, inplace=True)
# test_gbc.drop(['date_month','issuing_date_month'], axis=1, inplace=True)

train_gbc.head(3)

"""# imputation part

"""

mask = train_gbc.isnull().any(axis=1)
missing_values_df = train_gbc[mask]
missing_values_df

"""## numeric"""

def numeric_column_imputation (df,col,order_by_col,group_by_col,rolling_window_number):
  df = df.sort_values(order_by_col)
  df[col+'_imputed'] = df.groupby(group_by_col)[col].rolling(window=rolling_window_number, min_periods=1).mean().values
  df[col+'_imputed'] = df.groupby(group_by_col)[col+'_imputed'].fillna(method='bfill').fillna(method='ffill')

  mask = df[col].isnull()
  df.loc[mask,col] = df.loc[mask,col].fillna(df[col+'_imputed'])
  df = df.drop(col+'_imputed',axis = 1)

  return df

train_gbc = numeric_column_imputation(train_gbc,'transaction_amount',['customer_id', 'date'],'customer_id',4)
test_gbc = numeric_column_imputation(test_gbc,'transaction_amount',['customer_id', 'date'],'customer_id',4)

mask = train_gbc.isnull().any(axis=1)
missing_values_df = train_gbc[mask]
missing_values_df

"""#  categories"""

# def category_column_imputation(df,col,category_values):
#   df[col+'_num'] = pd.Categorical(df[col], categories=category_values, ordered=True).codes
#   df[col+'_num'] = df[col+'_num'].replace(-1, np.nan)
#   df.drop([col], axis=1, inplace=True)
#   return df

# pt_category_values = ['Basic','Standard','Premium']
# train_gbc = category_column_imputation(train_gbc,'plan_type',pt_category_values)
# test_gbc = category_column_imputation(test_gbc,'plan_type',pt_category_values)

# scaler = MinMaxScaler()
# for col in ['transaction_amount','customer_lifetime','date_year','issuing_date_year']:
#   train_gbc[col] = scaler.fit_transform(train_gbc[[col]])
#   test_gbc[col] = scaler.fit_transform(test_gbc[[col]])

# knn_imput = KNNImputer(n_neighbors=5)
# train_gbc_ind = train_gbc.index
# train_gbc = pd.DataFrame(knn_imput.fit_transform(train_gbc), columns=train_gbc.columns, index=train_gbc_ind)
# train_gbc['plan_type_num'] = train_gbc['plan_type_num'].round()
# test_gbc_ind = test_gbc.index
# test_gbc = pd.DataFrame(knn_imput.fit_transform(test_gbc), columns=train_gbc.columns, index=test_gbc_ind)
# test_gbc['plan_type_num'] = test_gbc['plan_type_num'].round()

most_frequent_by_group = train_gbc.groupby(['customer_id'])['plan_type'].agg(lambda x: pd.Series.mode(x).iloc[0] if not pd.Series.mode(x).empty else np.nan)
train_gbc = train_gbc.join(most_frequent_by_group, on='customer_id', rsuffix='_mode')
train_gbc['plan_type'] = train_gbc['plan_type'].fillna(train_gbc['plan_type_mode'])
train_gbc.head(3)

most_frequent_by_group = test_gbc.groupby(['customer_id'])['plan_type'].agg(lambda x: pd.Series.mode(x).iloc[0] if not pd.Series.mode(x).empty else np.nan)
test_gbc = test_gbc.join(most_frequent_by_group, on='customer_id', rsuffix='_mode')
test_gbc['plan_type'] = test_gbc['plan_type'].fillna(test_gbc['plan_type_mode'])
test_gbc.head(3)

train_gbc.head(3)

"""# out source enrichment"""

def financial_enrichment (fe_df,date_col,fred_data):
  start_date_available = fe_df[date_col].min()
  end_date_available = fe_df[date_col].max()
  financial_df = pdr.DataReader(fred_data, 'fred', start_date_available, end_date_available).reset_index()
  financial_df['DATE'] = pd.to_datetime(financial_df['DATE'])
  financial_df.columns = financial_df.columns.str.lower()
  fe_df['date'] = pd.to_datetime(fe_df['date'])
  fe_df = pd.merge(fe_df.reset_index(), financial_df, on='date', how='left').set_index(['customer_id', 'date_ind'])
  return fe_df

financial_info = ['FEDFUNDS','CPIAUCSL','GS3M']
# FEDFUNDS --> USA intrest
# CPIAUCSL --> USA consumer price index (for measuring inflation)
# GS3M --< government bonds 3 month
for fin in financial_info:
  train_gbc = financial_enrichment(train_gbc,'date',fin)
for fin in financial_info:
  test_gbc = financial_enrichment(test_gbc,'date',fin)

train_gbc

"""# data preparation and scaling"""

alternative_df_train = train_gbc.copy()
alternative_df_test = test_gbc.copy()

train_gbc = alternative_df_train.copy()
test_gbc = alternative_df_test.copy()
pt_category_values = ['Basic','Standard','Premium']
pt_enc = LabelEncoder()
train_gbc['plan_type_encoded'] = pt_enc.fit_transform(train_gbc['plan_type'])
test_gbc['plan_type_encoded'] = pt_enc.fit_transform(test_gbc['plan_type'])

train_gbc['transaction_amount_rolling'] = train_gbc.groupby('customer_id')['transaction_amount'].cumsum()
train_gbc['transaction_amount_rolling_per'] = train_gbc['transaction_amount'] / train_gbc.groupby('customer_id')['transaction_amount_rolling'].shift(1)
train_gbc['transaction_amount_rolling_per'] = train_gbc['transaction_amount_rolling_per'].fillna(0)
train_gbc['transaction_amount_rolling_life'] = train_gbc['transaction_amount_rolling_per'] * train_gbc['customer_lifetime']

test_gbc['transaction_amount_rolling'] = test_gbc.groupby('customer_id')['transaction_amount'].cumsum()
test_gbc['transaction_amount_rolling_per'] = test_gbc['transaction_amount'] / test_gbc.groupby('customer_id')['transaction_amount_rolling'].shift(1)
test_gbc['transaction_amount_rolling_per'] = test_gbc['transaction_amount_rolling_per'].fillna(0)
test_gbc['transaction_amount_rolling_life'] = test_gbc['transaction_amount_rolling_per'] * test_gbc['customer_lifetime']

train_gbc['plan_type_transaction'] = train_gbc['transaction_amount'] * train_gbc['plan_type_encoded']
train_gbc['plan_type_transaction_life'] = train_gbc['transaction_amount'] * train_gbc['plan_type_encoded'] * train_gbc['customer_lifetime']

test_gbc['plan_type_transaction'] = test_gbc['transaction_amount'] * test_gbc['plan_type_encoded']
test_gbc['plan_type_transaction_life'] = test_gbc['transaction_amount'] * test_gbc['plan_type_encoded'] * test_gbc['customer_lifetime']

# train_gbc = train_gbc.drop(['date','issuing_date','plan_type','plan_type_mode','date_month','issuing_date_month'],axis=1)
# test_gbc = test_gbc.drop(['date','issuing_date','plan_type','plan_type_mode','date_month','issuing_date_month'],axis=1)
train_gbc = train_gbc.drop(['date','issuing_date','plan_type','plan_type_mode','date_year','transaction_amount_rolling_life','transaction_amount_rolling_per','plan_type_transaction_life','plan_type_transaction','date_month_sin','date_month_cos','date_month','issuing_date_month'],axis=1)
test_gbc = test_gbc.drop(['date','issuing_date','plan_type','plan_type_mode','date_year','transaction_amount_rolling_life','transaction_amount_rolling_per','plan_type_transaction_life','plan_type_transaction','date_month_sin','date_month_cos','date_month','issuing_date_month'],axis=1)

scaler = MinMaxScaler()
for col in ['transaction_amount','customer_lifetime','issuing_date_year','fedfunds','cpiaucsl','gs3m','issuing_date_month_sin','issuing_date_month_cos','transaction_amount_rolling']:
  train_gbc[col] = scaler.fit_transform(train_gbc[[col]])
  test_gbc[col] = scaler.fit_transform(test_gbc[[col]])

train_gbc = train_gbc.reset_index()
train_gbc = train_gbc.drop(['customer_id','date_ind'],axis=1)
test_gbc = test_gbc.reset_index()
test_gbc = test_gbc.drop(['customer_id','date_ind'],axis=1)

train_gbc.head(3)

test_gbc.head(3)

"""# Models"""

train_gbc_churn.value_counts()

# smote_tomek = SMOTETomek(random_state=42)
# train_gbc_balanced, train_gbc_churn_balanced = smote_tomek.fit_resample(train_gbc, train_gbc_churn)

smote = SMOTE(random_state=42)
train_gbc_balanced, train_gbc_churn_balanced = smote.fit_resample(train_gbc, train_gbc_churn)

train_gbc_churn_balanced.value_counts()

"""# Logistic Regession Model"""

param_grid = {
    'C': [0.1, 0.5, 1, 10],
    'penalty': ['l1', 'l2']
}
logi = LogisticRegression(solver='liblinear', random_state=42)
grid_search = GridSearchCV(logi, param_grid, cv=5, scoring='f1_weighted')
grid_search.fit(train_gbc, train_gbc_churn)
lr_best_model = grid_search.best_estimator_

y_pred = lr_best_model.predict(test_gbc)
y_prob = lr_best_model.predict_proba(test_gbc)[:, 1]

print(classification_report(test_gbc_churn, y_pred))
print("AUC:", roc_auc_score(test_gbc_churn, y_prob))
print("F1 score (test):", f1_score(test_gbc_churn, y_pred, average='weighted'))

logi_reg_f1 = f1_score(test_gbc_churn, y_pred, average='weighted')


kf = KFold(n_splits=5, shuffle=True, random_state=42)
f1_scores = []

for train_index, test_index in kf.split(train_gbc):
    X_train, X_test = train_gbc.iloc[train_index], train_gbc.iloc[test_index]
    y_train, y_test = train_gbc_churn.iloc[train_index], train_gbc_churn.iloc[test_index]
    lr_best_model.fit(X_train, y_train)
    y_pred = lr_best_model.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)

print("F1 scores (validation):", f1_scores)

logistic_model = LogisticRegression(solver='liblinear', random_state=42)
logistic_model.fit(train_gbc, train_gbc_churn)
explainer = shap.Explainer(logistic_model, train_gbc)
shap_values = explainer(train_gbc)
shap.summary_plot(shap_values, train_gbc, feature_names=train_gbc.columns)

y_pred = logistic_model.predict(train_gbc)
print(classification_report(train_gbc_churn, y_pred))

y_pred = logistic_model.predict(test_gbc)
print(classification_report(test_gbc_churn, y_pred))

logistic_model.fit(train_gbc_balanced, train_gbc_churn_balanced)
explainer = shap.Explainer(logistic_model, train_gbc_balanced)
shap_values = explainer(train_gbc_balanced)
shap.summary_plot(shap_values, train_gbc_balanced, feature_names=train_gbc_balanced.columns)

y_pred = logistic_model.predict(train_gbc)
print(classification_report(train_gbc_churn, y_pred))

y_pred = logistic_model.predict(test_gbc)
print(classification_report(test_gbc_churn, y_pred))

"""# Random Forest Classifier Model"""

param_grid = {
    'n_estimators': [200, 250, 300],
    'max_depth': [2, 3, 4],
    'min_samples_split': [40, 50, 60],
    'min_samples_leaf': [5, 10, 15]
                  }
rfc = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=10, scoring='f1')
grid_search.fit(train_gbc, train_gbc_churn)
rf_best_model = grid_search.best_estimator_

kf = KFold(n_splits=5, shuffle=True, random_state=42)
f1_scores = []
for train_index, test_index in kf.split(train_gbc):
    X_train, X_test = train_gbc.iloc[train_index], train_gbc.iloc[test_index]
    y_train, y_test = train_gbc_churn.iloc[train_index], train_gbc_churn.iloc[test_index]

    y_pred = rf_best_model.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)

y_pred = rf_best_model.predict(test_gbc)
rf_f1 = f1_score(test_gbc_churn, y_pred, average='weighted')
print("F1 score (test):", rf_f1)

"""# XGboost Model"""

X_train_val, X_val, y_train_val, y_val = train_test_split(train_gbc, train_gbc_churn, test_size=0.3, random_state=42)



xgb = XGBClassifier(random_state=42)
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [4, 5, 6],
    'learning_rate': [0.1, 0.2,0.3],
    'reg_alpha': [0, 0.1, 0.2],
    'reg_lambda': [0,0.5,1]
}
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=10, scoring='f1_weighted')
grid_search.fit(train_gbc, train_gbc_churn)
xgb_best_model = grid_search.best_estimator_


kf = KFold(n_splits=5, shuffle=True, random_state=42)
f1_scores = []
for train_index, test_index in kf.split(train_gbc):
    X_train, X_test = train_gbc.iloc[train_index], train_gbc.iloc[test_index]
    y_train, y_test = train_gbc_churn.iloc[train_index], train_gbc_churn.iloc[test_index]
    xgb_best_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])

    y_pred = xgb_best_model.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)

y_pred = xgb_best_model.predict(test_gbc)
xgb_f1 = f1_score(test_gbc_churn, y_pred, average='weighted')
print("F1 score (test):", xgb_f1)

if logi_reg_f1 >= rf_f1 and logi_reg_f1 >= xgb_f1:
  best_f1 = logi_reg_f1
  best_model_name = 'Logistic regression'
  best_model = lr_best_model
elif rf_f1 >= logi_reg_f1 and rf_f1 >= xgb_f1:
  best_f1 = rf_f1
  best_model_name = 'Randim forest'
  best_model = rf_best_model
else:
  best_f1 = xgb_f1
  best_model_name = 'XGBoost'
  best_model = xgb_best_model

print(best_f1)
print(best_model_name)

CSV_include_prediction = df_origianl.copy()
df_include_prediction = pd.concat([train_gbc, test_gbc], ignore_index=True)
predictions = best_model.predict(df_include_prediction)
df_include_prediction['predictions'] = predictions
CSV_include_prediction['predictions'] = df_include_prediction['predictions']
CSV_include_prediction

df.to_csv('/content/drive/MyDrive/Atidot_home_assignment/CSV_include_prediction.csv',index=True)

with open('/content/drive/MyDrive/Atidot_home_assignment/Atidot_home_assignment_model.pkl','wb') as f:
  pickle.dump(best_model,f)